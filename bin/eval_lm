#!/usr/bin/env python

import argparse
from collections import defaultdict
import os

from datasets import load_dataset
import numpy as np
from tokenizers import Tokenizer
import torch
from torch.utils.data.dataloader import DataLoader
from torch.nn import functional as F
from transformers import default_data_collator
from transformers.tokenization_utils_base import BatchEncoding

from languagemodels import LMFactory, TokenizerFactory
from languagemodels.scripting_utils import (
    get_num_occurrences_in_tensor,  
    repackage_hidden,
    ids_to_tokens
)
from languagemodels.utils import load_args


def parse_args():
    parser = argparse.ArgumentParser(description="languagemodels-eval")

    # data args
    parser.add_argument('--test_file', type=str, 
                        help="path to test data")

    # model weights, model config, command line args
    parser.add_argument('--model_dir', type=str, 
                        help="path to model weights, config and trainig command line args")
    parser.add_argument('--device', type=str, default='cpu',
                        help="device to use for compute, examples: cpu|cuda|cuda:0")

    # output dir
    parser.add_argument('--output_dir', type=str, 
                        help="path to the saved evaluation results")
    # Flags for saving different parts of the output (surprisal & tokens always saved)
    parser.add_argument("--save_word_ids", action='store_true')
    parser.add_argument("--save_word_offsets", action='store_true')

    # testing args
    parser.add_argument('--batch_size', type=int, default=10,
                        help="batch size")
    
    args = parser.parse_args()
    return args
    

def main():
    args = parse_args()

    train_args = load_args(args.model_dir)

    assert os.path.exists(args.model_dir)

    data_files = {
        "test": args.test_file
    }

    if not os.path.exists(args.output_dir):
        os.makedirs(args.output_dir)

    dataset_args = {}
    extension = args.test_file.split(".")[-1]
    if extension == "txt":
        extension = "text"

    config_path = os.path.join(args.model_dir, "config.json")
    model_path = os.path.join(args.model_dir, "pytorch_model.bin")
    tokenizer_path = os.path.join(args.model_dir, "tokenizer")
    
    raw_dataset = load_dataset(extension, data_files=data_files, **dataset_args)

    tokenizer = TokenizerFactory.get_tokenizer(tokenizer_type=train_args.tokenizer_type, \
        tokenizer_name=train_args.tokenizer_name, tokenizer_name_or_path=tokenizer_path)

    # This is important because padded positions will be removed from the output dict
    pad_id = tokenizer.token_to_id("<pad>") if isinstance(tokenizer, Tokenizer) else tokenizer.pad_token_id

    # load model
    model, model_config = LMFactory.get_lm(model_type=train_args.model_type, config_name_or_path=config_path,
                                           pre_trained=True, model_name_or_path=model_path)
    model.eval()
    model.to(args.device)

    # group data into chunks of length block_size (block_size will be model specific, e.g. BigramLM)
    block_size = 512 if train_args.model_type == "bert" else model.block_size
    # the stride is an important assumption we make on the format of our input data
    stride = 1 if train_args.model_type == "bigram-lm" else block_size

    # tokenize and encode data
    def tokenize_function(sequences):
        # we tokenize in batch mode, hence sequences will be batch_size many sequences
        # the text field holds the input text
        # map expects a dictionary
        output_dict = {"input_ids": [],
                       "tokens": [], "attention_mask": [], "special_tokens_mask": [], "word_ids": []}
        outputs = tokenizer.encode_batch(
            sequences["text"])  # returns an Encoding object (https://huggingface.co/docs/tokenizers/v0.13.0/en/api/encoding#tokenizers.Encoding)
        
        # Output of CharacterBasedTokenizer.encode_batch is already in the correct format
        if isinstance(outputs, BatchEncoding):
            return outputs
        else:
            for output in outputs:
                output_dict["input_ids"].append(output.ids)
                output_dict["tokens"].append(output.tokens)
                output_dict["word_ids"].append(output.word_ids)
                output_dict["attention_mask"].append(output.attention_mask)
                output_dict["special_tokens_mask"].append(
                    output.special_tokens_mask)

        return output_dict


    def add_labels(sequences):
        # concatenate all sequences into a single sequence
        # concatenated_examples = {
        #     k: list(chain(*sequences[k])) for k in sequences.keys()}
        # concatenated_examples["text"] = "".join(
        #     concatenated_examples["text"]).split()

        # actual_total_length = len(concatenated_examples["input_ids"])

        # make sure data is divisible by block_size
        # as a result, we might ingore some tokens at the end of the sequence
        # TODO(mm): pad the last sequence instead
        # if actual_total_length >= block_size:
        #     total_length = (actual_total_length // block_size) * block_size
        #     print(
        #         f"Deleting {actual_total_length - total_length} tokens")

        # group sequences into blocks of length block_size
        # depending on the stridge, these blocks might be overlapping or not
        # e.g. assume our sequence is <s> A B C D E F G </s> and we have a block_size of 2 and a stride of 1
        # --> [<s> A] [A B] [B C] [C D] [D E] [F G] [G </s>]
        # result = {
        #     k: [values[i: i + block_size]
        #         for i in range(0, total_length, stride)]
        #     for k, values in concatenated_examples.items()
        # }
        # all models will shift the labels, so here it's fine to simply copy the inputs
        # for the bigram LM this is a bit of a waste of memory but we ignore this for now
        # result["labels"] = result["input_ids"].copy()
        
        # return result

        sequences["labels"] = sequences["input_ids"].copy()
        return sequences


    tokenized_datasets = raw_dataset.map(
        tokenize_function,
        batched=True,
        batch_size=args.batch_size,
        num_proc=1,  # no parallel processing for now
        desc="Running tokenizer on datasets"
    )

    lm_datasets = tokenized_datasets.map(
        add_labels,
        batched=True,
        batch_size=1000,  # currently we group groups of 1000 samples together. For each of these groups we might delete some tokens
        num_proc=1,  # no parallel processing for now
        desc="adding labels"
    )

    test_dataset = lm_datasets["test"].with_format(
        type="torch", columns=["input_ids", "labels", "attention_mask", \
            "special_tokens_mask", ])


    predicted_tokens = 0
    hidden_state = None

    for i in range(10):
        print(test_dataset[i])

    test_dataloader = DataLoader(
            test_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=args.batch_size, drop_last=True)

    out_file_path = os.path.join(args.output_dir, f"results.tsv")
    print(f"Saving evaluation results to {out_file_path}")
    
    with open(out_file_path, "w") as f:
        for batch_id, batch in enumerate(test_dataloader):
            
            actual_sequence_length = batch["input_ids"].shape[-1]
            assert actual_sequence_length == block_size, \
                f"Expected tokenized inputs to be of shape ({args.batch_size},{block_size}), got ({args.batch_size},{actual_sequence_length})"

            # init output dict
            out_dict = defaultdict(list)

            # put data on device
            batch = {k: v.to(args.device) for k, v in batch.items()}

            # run forward pass and compute loss
            model.zero_grad(set_to_none=True)
            if train_args.model_type == "bert":
                logits, loss, final_hidden_state = model.forward(
                    input_ids=batch["input_ids"])
            else:
                logits, loss, final_hidden_state = model.forward(
                        input_ids=batch["input_ids"], labels=batch["labels"], hidden_state=hidden_state)

            if final_hidden_state is not None:
                # detach the hidden state to avoid backpropagating through all batches
                hidden_state = repackage_hidden(final_hidden_state)

            if train_args.model_type == "bert":
                    mask_count = get_num_occurrences_in_tensor(tokenizer.mask_token_id, batch["labels"])
                    predicted_tokens += mask_count
            else:
                predicted_tokens += args.batch_size * (block_size - 1)

            probs = F.softmax(logits, dim=-1)

            word_ids = batch["labels"].reshape(args.batch_size, block_size, 1)
            surprisals = -torch.log2(probs)
            surprisals = torch.gather(surprisals, -1, word_ids)
            surprisals = surprisals.squeeze().detach().cpu().numpy()

            # Keep only surprisal values from unpadded positions. This is done by multiplying
            # the surprisal values by the attention mask, which is 0 at padded positions. 
            mask = batch["attention_mask"].detach().cpu().numpy()
            surprisals = [(s*mask) for s, mask in zip(surprisals, mask)]
            surprisals = [list(s[np.where(s!=0)]) for s in surprisals]
            
            # remove pad ids
            word_ids = [[int(id) for id in ids if id != pad_id] for ids in word_ids.squeeze().cpu().numpy()]
            words = [ids_to_tokens(ids, tokenizer) for ids in word_ids]
            
            # special_tokens_mask = batch["special_tokens_mask"].cpu().numpy()

            for ids, toks, surp in zip(word_ids, words, surprisals):
                assert len(ids) == len(toks)
                assert len(ids) == len(toks)
            
            out_dict["surprisal"].extend(surprisals)
            out_dict["tokens"].extend(words)
            # out_dict["special_tokens_mask"].extend(special_tokens_mask)

            if args.save_word_ids:
                out_dict["word_ids"].extend(word_ids)

            if args.save_word_offsets:
                # TODO (js) gather & save word offsets
                pass

            # save results to tsv
            f.write("token_index\tsent_index\ttoken\ttoken_id\tsurprisal\n")
            for i, (sent_tokens, sent_surprisal, sent_ids) in enumerate(zip(words, surprisals, word_ids)):
                for j, (tok, srp, id) in enumerate(zip(sent_tokens, sent_ids, sent_surprisal)):
                    out_str = f"{j}\t{i}\t{tok}\t{id}\t{srp}\n"
                    f.write(out_str)

if __name__ == "__main__":
    main()