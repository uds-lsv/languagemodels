{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set current path\n",
    "PATH = !pwd\n",
    "PATH = PATH[0]\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/nethome/mmosbach/projects/languagemodels/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download a dateset\n",
    "!wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip -P {PATH}/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip the data\n",
    "!unzip {PATH}/datasets/wikitext-103-raw-v1.zip -d {PATH}/datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the data first\n",
    "with open(f\"{PATH}/datasets/wikitext-103-raw/wiki.valid.raw\", encoding=\"utf-8\") as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "print(data[:10]) # let's print the first 10 lines. notice that a line can contain one more sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's train a word level tokenizer on the data\n",
    "tokenizer = Tokenizer(model=WordLevel(unk_token=\"<unk>\"))\n",
    "trainer = WordLevelTrainer(vocab_size=30000, special_tokens=[\"<unk>\", \"<s>\", \"</s>\", \"<pad>\"]) # the special_tokens will be usueful later\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "files = [f\"{PATH}/datasets/wikitext-103-raw/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]]\n",
    "tokenizer.train(files, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the trained tokenizer\n",
    "tokenizer.save(f\"{PATH}/datasets/wikitext-103-raw/tokenizer-wiki.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's load our trained tokenizer\n",
    "tokenizer = Tokenizer.from_file(f\"{PATH}/datasets/wikitext-103-raw/tokenizer-wiki.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's tokenize and encode some text\n",
    "output = tokenizer.encode(\"Hello, y'all! How are you üòÅ ?\")\n",
    "print(output)\n",
    "print(output.ids)\n",
    "print(output.tokens)\n",
    "print(output.offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use a post processor to surround each sequence with special symbols \n",
    "tokenizer.post_processor = TemplateProcessing(single=\"<s> $A </s>\", special_tokens=[(t, tokenizer.token_to_id(t)) for t in [\"<s>\", \"</s>\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's tokenize the same text again\n",
    "output = tokenizer.encode(\"Hello, y'all! How are you üòÅ ?\")\n",
    "print(output)\n",
    "print(output.ids)\n",
    "print(output.tokens)\n",
    "print(output.offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's encode a few sequences from our data\n",
    "outputs = tokenizer.encode_batch(data[:10])\n",
    "for output in outputs:\n",
    "    # print(output)\n",
    "    print(output.ids)\n",
    "    print(output.tokens)\n",
    "    print(output.offsets)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's enable padding (to make sure all encoded sequences have at least n tokens)\n",
    "# NOTE: auto-regressive LMs typically do not use padding tokens\n",
    "tokenizer.enable_padding(pad_id=tokenizer.token_to_id(\"<pad>\"), pad_token=\"<pad>\", length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's again encode a few sequences from our data\n",
    "outputs = tokenizer.encode_batch(data[:10])\n",
    "for output in outputs:\n",
    "    # print(output)\n",
    "    print(len(output.ids))\n",
    "    print(output.ids)\n",
    "    print(output.attention_mask) # takes padding into account\n",
    "    print(output.tokens)\n",
    "    print(output.offsets)\n",
    "    print()\n",
    "\n",
    "# --> based on this we we create batches to train our language model on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from languagemodels.tokenization import train_word_level_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f\"{PATH}/datasets/wikitext-103-raw/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]]\n",
    "\n",
    "tokenizer = train_word_level_tokenizer(vocab_size=1000, files=files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create some toy data\n",
    "data = [\n",
    "    \"This is the first example sentence .\",\n",
    "    \"Another one, but shorter .\"\n",
    "]\n",
    "\n",
    "outputs = tokenizer.encode_batch(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for output in outputs:\n",
    "    # print(output)\n",
    "    print(len(output.ids))\n",
    "    print(output.ids)\n",
    "    print(output.attention_mask) # takes padding into account\n",
    "    print(output.tokens)\n",
    "    print(output.offsets)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 ('languagemodels-venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b386bb3b014ad8a2d1d02e698168a948288368d262288f15eea50f948acc9410"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
